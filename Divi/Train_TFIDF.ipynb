{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "Total # of entries in train data: 6034195\n",
      "Total # of keywords in train data (remove repeat one): 42048\n",
      "Total frequency of all 42036 keywords: 17409994 (100%)\n",
      "Top 10000 keywords frequency: 16656503 (95.67%)\n",
      "Top 1000 keywords frequency: 12454534 (71.54%)\n",
      "Top 100 keywords frequency: 7135915 (40.99%)\n",
      "Top 10 keywords frequency: 3005140 (17.26%)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#\n",
      "# Training variable definition\n",
      "#\n",
      "\n",
      "\n",
      "TOTAL_LOADING_LINE = 100000\n",
      "TRAINING_LINE_NUMBER = 1000\n",
      "\n",
      "IDF_LINE_NUMBER = 500\n",
      "\n",
      "TOP_KEYWORDS_FROM = 0\n",
      "TOP_KEYWORDS_TO = 5\n",
      "TOP_WORDS = 50\n",
      "\n",
      "BIGRAMS = False\n",
      "\n",
      "\n",
      "\"\"\" Tool function \"\"\"\n",
      "\n",
      "\n",
      "#\n",
      "# find lines contain keys\n",
      "#\n",
      "\n",
      "def get_line_num_by_key(key):\n",
      "    line_num = []\n",
      "    for i in range(len(keyword_list)):\n",
      "        if key in keyword_list[i].split(\" \"):\n",
      "            line_num.append(i)\n",
      "    return line_num\n",
      "\n",
      "\n",
      "#\n",
      "# Data Cleaner\n",
      "#\n",
      "import re\n",
      "\n",
      "def data_cleaner(raw_text):\n",
      "    text = re.sub(\"<p>|</p>|<pre>|<code>|</pre>|</code>\", '', raw_text)\n",
      "    text = re.sub(\"\\n|[,!;?:/']\", ' ', text)\n",
      "    text = text.split(\" \")\n",
      "    li = []\n",
      "    for w in text:\n",
      "        w = w.lower()\n",
      "        if w != \"\":\n",
      "            if w[-1] == \".\":\n",
      "                w = w[:-1]\n",
      "            li.append(w)\n",
      "    return li\n",
      "\n",
      "\n",
      "#\n",
      "# Code extractor\n",
      "#\n",
      "\n",
      "def code_extractor(raw_text):\n",
      "    m = re.findall('<pre><code>(.+?)</code></pre>', raw_text, re.S)\n",
      "    return m\n",
      "\n",
      "\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "#\n",
      "# Load train set data\n",
      "#\n",
      "\n",
      "import csv\n",
      "import nltk\n",
      "from nltk import bigrams\n",
      "import time\n",
      "\n",
      "skip_first_line = True\n",
      "\n",
      "input_file = open(\"C:\\NLP_Data\\Train\\small_train.csv\",'r')\n",
      "reader = csv.reader( input_file )\n",
      "if skip_first_line:\n",
      "    reader.next()\n",
      "\n",
      "#id_list = []\n",
      "title_list = []\n",
      "question_list = []\n",
      "keyword_list = []\n",
      "\n",
      "i = 0\n",
      "\n",
      "for line in reader:\n",
      "#    id_list.append(line[0])\n",
      "    title_list.append(line[1])\n",
      "    question_list.append(line[2])\n",
      "    keyword_list.append(line[3])\n",
      "    i+=1\n",
      "    if i == TOTAL_LOADING_LINE:\n",
      "        break\n",
      "\n",
      "        \n",
      "#\n",
      "# Build keyword freq table\n",
      "#\n",
      "fq = nltk.FreqDist([keyword for keywords in keyword_list for keyword in keywords.split(\" \")])\n",
      "\n",
      "\n",
      "#\n",
      "# Build tfidf table\n",
      "# \n",
      "import pickle\n",
      "tagger = pickle.load(open(\"treebank_brill_aubt.pickle\"))\n",
      "\n",
      "from nltk.corpus import stopwords \n",
      "\n",
      "freq_word = {}\n",
      "\n",
      "# build tf\n",
      "\n",
      "# for time checking\n",
      "start_time = time.time()\n",
      "#\n",
      "\n",
      "for keyword in fq.keys()[TOP_KEYWORDS_FROM:TOP_KEYWORDS_TO]:\n",
      "\n",
      "    line_num = get_line_num_by_key(keyword)\n",
      "    target_words = []\n",
      "    for num in line_num:\n",
      "        text = data_cleaner(question_list[num])\n",
      "        \n",
      "        if (not BIGRAMS):\n",
      "            # Unigram\n",
      "            text = tagger.tag(text)\n",
      "            for (word,tag) in text:\n",
      "                if (word not in stopwords.words('english'))and tag in [\"JJ\",\"NN\", \"NNP\", \"NNS\",\"-None-\"] and word not in [\"\",\"=\",\"{\",\"}\",\"(\",\")\",\"+\",\"==\"]:\n",
      "                    target_words.append(word.lower())\n",
      "        else:\n",
      "            # Bigram\n",
      "            text = bigrams(text)\n",
      "            for word1,word2 in text:\n",
      "                target_words.append(word1.lower() + \" \" + word2.lower())\n",
      "                \n",
      "    fdist = nltk.FreqDist(target_words)    \n",
      "    key_value = {}\n",
      "    for key in fdist.keys()[:TOP_WORDS]:\n",
      "        key_value[key] = [fdist[key],0]\n",
      "    freq_word[keyword] = key_value\n",
      "\n",
      "    \n",
      "# for time checking\n",
      "elapsed_time = time.time() - start_time\n",
      "print \"TF:\",elapsed_time\n",
      "#    \n",
      "\n",
      "\n",
      "# build idf\n",
      "\n",
      "# for time checking\n",
      "start_time = time.time()\n",
      "#\n",
      "for i in range(IDF_LINE_NUMBER):\n",
      "\n",
      "    keywords = keyword_list[i].split(\" \")\n",
      "    text = data_cleaner(question_list[i])\n",
      "    if (not BIGRAMS):\n",
      "    # Unigram\n",
      "        for keyword in freq_word.keys():\n",
      "            if keyword not in keywords:\n",
      "                for word in text:\n",
      "                    if word in freq_word[keyword].keys():\n",
      "                        freq_word[keyword][word][1] += 1\n",
      "    else:\n",
      "    # Bigram\n",
      "        text = bigrams(text)\n",
      "        for keyword in freq_word.keys():\n",
      "            if keyword not in keywords:\n",
      "                for word1,word2 in text:\n",
      "                    word = word1.lower() + \" \" + word2.lower()\n",
      "                    if word in freq_word[keyword].keys():\n",
      "                        freq_word[keyword][word][1] += 1\n",
      "                    \n",
      "                    \n",
      "# for time checking\n",
      "elapsed_time = time.time() - start_time\n",
      "print \"IDF:\",elapsed_time\n",
      "#  "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "IOError",
       "evalue": "[Errno 2] No such file or directory: 'treebank_brill_aubt.pickle'",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mIOError\u001b[0m                                   Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-2-c9f1f5e1eed2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m \u001b[0mtagger\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"treebank_brill_aubt.pickle\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'treebank_brill_aubt.pickle'"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#\n",
      "# Output tfidf table\n",
      "#\n",
      "\n",
      "\n",
      "import numpy\n",
      "from __future__ import division\n",
      "\n",
      "\n",
      "if (not BIGRAMS):\n",
      "    f = open(\"tfidf_\"+str(TRAINING_LINE_NUMBER)+\"_\"+str(TOP_KEYWORDS_FROM)+\"_\"+str(TOP_KEYWORDS_TO)+\"_\"+str(TOP_WORDS),\"w\")\n",
      "\n",
      "else:\n",
      "    f = open(\"tfidf_bi_\"+str(TRAINING_LINE_NUMBER)+\"_\"+str(TOP_KEYWORDS_FROM)+\"_\"+str(TOP_KEYWORDS_TO)+\"_\"+str(TOP_WORDS),\"w\")\n",
      "\n",
      "for keyword in fq.keys()[TOP_KEYWORDS_FROM:TOP_KEYWORDS_TO]:\n",
      "    for word in freq_word[keyword].keys():\n",
      "        tf = freq_word[keyword][word][0]\n",
      "        idf = freq_word[keyword][word][1]\n",
      "        if word[0] != '\"':\n",
      "            f.write(str(keyword) + \"\\t\" + str(word) + \"\\t\" + str(tf/(idf+1)) + \"\\t\" + str(tf) + \"\\t\" + str(idf) + \"\\t\" +str(fq[keyword])+ \"\\n\")\n",
      "f.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 33
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}